import os
import pandas as pd
import numpy as np
from flask import request, jsonify
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from config import UPLOAD_FOLDER
from utils import read_df


def do_summary():
    """
    ã€æ•°æ®å®è§‚æ¢æµ‹å™¨ã€‘ï¼šè¯„ä¼°æ•°æ®è´¨é‡ï¼Œè®¡ç®—ç¨€ç–åº¦ï¼Œå¹¶åŸºäºçš®å°”é€Šç›¸å…³æ€§æä¾› AI æ¨¡å‹è®­ç»ƒçš„è‡ªåŠ¨åŒ–å»ºè®®é¶ç‚¹ã€‚
    """
    try:
        filename = request.json.get('filename')
        if not filename: return jsonify({"status": "error", "message": "ç³»ç»Ÿæœªæ‰¾åˆ°å½“å‰å¤„ç†çš„æ–‡ä»¶"}), 400
        df = read_df(os.path.join(UPLOAD_FOLDER, filename))

        # å°è¯•å°†èƒ½è½¬æ•°å€¼çš„åˆ—éƒ½å¼ºè½¬æˆæ•°å€¼ï¼Œä¸ºæœºå™¨å­¦ä¹ é“ºè·¯
        for col in df.columns:
            try:
                df[col] = pd.to_numeric(df[col])
            except Exception:
                pass

        # æ ¸å¿ƒç‰¹å¾åˆ†ç±»
        all_cols = df.columns.tolist()
        numeric_cols = [c for c in all_cols if pd.api.types.is_numeric_dtype(df[c]) and not any(
            kw in str(c).lower() for kw in ['å·', 'id', 'ç¼–å·', 'ä»£ç '])]
        categorical_cols = [c for c in all_cols if c not in numeric_cols]
        binary_cols = [c for c in all_cols if df[c].nunique() == 2]

        row_count = len(df)
        col_count = len(df.columns)
        insights = []

        # 1. çŸ©é˜µç¨€ç–åº¦ (Sparsity) æ¢é’ˆä¸è¿‡æ‹Ÿåˆé¢„è­¦
        total_cells = row_count * col_count
        missing_cells = df.isnull().sum().sum()
        missing_rate = missing_cells / total_cells if total_cells > 0 else 0
        quality_score = 100 - (missing_rate * 100)

        if row_count < 30:
            quality_desc = f"âš ï¸ **æ•°æ®è´¨é‡è¯„ä¼°**ï¼šè§„æ¨¡æå°ï¼ˆä»…åŒ…å« {row_count} æ¡è®°å½•ï¼‰ã€‚åœ¨æ­¤æ•°æ®é‡ä¸‹ææ˜“å‡ºç°ä¸¥é‡çš„â€œè¿‡æ‹Ÿåˆâ€ï¼Œå»ºè®®æ‚¨ä½¿ç”¨åŸºç¡€ã€Œæè¿°ç»Ÿè®¡ã€ã€‚"
        elif missing_rate > 0.15:
            quality_desc = f"âš ï¸ **æ•°æ®è´¨é‡è¯„ä¼°**ï¼šçŸ©é˜µå­˜åœ¨è¾ƒé«˜ç¨€ç–ç‡ï¼ˆç©ºå€¼å æ¯”é«˜è¾¾ {missing_rate * 100:.1f}%ï¼‰ã€‚å»ºè®®æ‚¨å…ˆè¿›è¡Œã€âœ¨ æ™ºèƒ½æ¸…æ´—ã€‘ä»¥å¡«è¡¥ç¼ºå¤±ç¼éš™ã€‚"
        else:
            quality_desc = f"ğŸ›¡ï¸ **æ•°æ®è´¨é‡è¯„ä¼°**ï¼šæä½³ï¼æ•°æ®çŸ©é˜µå®Œæ•´åº¦é«˜è¾¾ {quality_score:.1f}%ï¼Œå®Œå…¨å…·å¤‡å¼€å±•æ·±å±‚æœºå™¨å­¦ä¹ çš„åº•å±‚æ¡ä»¶ã€‚"

        insights.append(quality_desc)
        insights.append(
            f"ğŸ“¦ **å…¨å±€ç»“æ„å‰–æ**ï¼šç³»ç»ŸæˆåŠŸè§£æå‡º **{len(numeric_cols)}** ä¸ªã€Œé‡åŒ–æ•°å€¼æŒ‡æ ‡ã€ä¸ **{len(categorical_cols)}** ä¸ªã€Œå®šæ€§ç±»åˆ«æŒ‡æ ‡ã€ã€‚")

        # 2. ååº¦ (Skewness) ä¸åˆ†å¸ƒæ–­æ¡£æ£€æµ‹å™¨
        if len(numeric_cols) > 0:
            skew_insights = []
            for col in numeric_cols:
                skew_val = df[col].skew()
                if pd.notna(skew_val):
                    if skew_val > 1.5:
                        skew_insights.append(f"ã€{col}ã€‘å­˜åœ¨ä¸¥é‡çš„â€œå³åâ€æ–­æ¡£")
                    elif skew_val < -1.5:
                        skew_insights.append(f"ã€{col}ã€‘å­˜åœ¨ä¸¥é‡çš„â€œå·¦åâ€æ–­æ¡£")
            if skew_insights:
                insights.append("ğŸ“‰ **æç«¯åˆ†å¸ƒé¢„è­¦**ï¼šä¾¦æµ‹åˆ°éƒ¨åˆ†ç‰¹å¾å­˜åœ¨ä¸å¹³æ»‘çš„æ–­æ¡£åˆ†å¸ƒã€‚å…¶ä¸­ï¼š" + "ï¼›".join(
                    skew_insights) + "ã€‚å»ºè®®ä½¿ç”¨ 3Ïƒ å‡†åˆ™æ¸…æ´—å¼‚å¸¸é«˜é¢‘å€¼ã€‚")
            else:
                insights.append("ğŸ“Š **æ•°æ®åˆ†å¸ƒå¥åº·åº¦**ï¼šå„æ•°å€¼å‹æŒ‡æ ‡åˆ†å¸ƒå‡åŒ€ï¼Œå‘ˆç°è‰¯å¥½çš„å½¢æ€ç‰¹å¾ï¼Œæœªè§ä¸¥é‡åæ€ã€‚")

        if binary_cols:
            insights.append(
                f"âš–ï¸ **ç¾¤ä½“å·®å¼‚æ€§æ£€éªŒæ¨è**ï¼šä¾¦æµ‹åˆ°å®Œç¾çš„äºŒåˆ†ç±»å®šæ€§å˜é‡ã€{binary_cols[0]}ã€‘ï¼Œæå…¶é€‚åˆä½œä¸ºã€Œç‹¬ç«‹æ ·æœ¬ t æ£€éªŒã€çš„åˆ†ç»„åŸºå‡†æ¡ä»¶ã€‚")

        # 3. é¶ç‚¹ç‰¹å¾æ¨èç®—æ³• (åŸºäºå…³è”åº¦æå€¼æœå¯»æœ€æ˜“é¢„æµ‹ç‰¹å¾)
        has_ai_recommendation = False
        if len(numeric_cols) >= 3:
            corr_df = df[numeric_cols].corr().abs()
            target_candidate = corr_df.sum().idxmax()
            feature_candidates = [c for c in numeric_cols if c != target_candidate]
            insights.append(
                f"ğŸ¤– **AI é¢„æµ‹å¼•æ“æœ€ä½³è®­ç»ƒç»„åˆ**ï¼šä¸ºäº†è·å¾—æœ€ä½³å›å½’æ‹Ÿåˆæ•ˆæœï¼Œå»ºè®®å°†å…³è”åº¦æœ€é«˜çš„ã€{target_candidate}ã€‘è®¾ä¸º **ç›®æ ‡å˜é‡ (Y)**ï¼Œå¹¶å°†ã€{', '.join(feature_candidates[:3])}ã€‘ç­‰è®¾ä¸ºè§£é‡Šç‰¹å¾ï¼")
            has_ai_recommendation = True

        sop_text = "ğŸ—ºï¸ **é›¶åŸºç¡€ä¸“å±Â·æç®€åˆ†æå¯¼èˆª**ï¼š<br>ğŸ‘‰ **Step 1 æ¢åº•**ï¼šå‹¾é€‰æ•°å€¼å˜é‡ç‚¹å‡»ã€Œæè¿°ç»Ÿè®¡ã€ã€‚<br>ğŸ‘‰ **Step 2 å¯»å› **ï¼šå‹¾é€‰å˜é‡ç‚¹å‡»ã€Œå…³è”åˆ†æã€ã€‚"
        if has_ai_recommendation:
            sop_text += "<br>ğŸ‘‰ **Step 3 é¢„æµ‹**ï¼šè¿›å…¥ã€Œæœºå™¨å­¦ä¹ å¼•æ“ã€ä¸€é”®ç‚¹å‡»è®­ç»ƒï¼Œäº«å— AI è‡ªåŠ¨æ¨ç†çš„ä¹è¶£ï¼"
        else:
            sop_text += "<br>ğŸ‘‰ **Step 3 æ¢ç´¢**ï¼šè¿›å…¥ã€Œæœºå™¨å­¦ä¹ å¼•æ“ã€æŒ–æ˜æ›´æ·±å±‚çš„è§„å¾‹ã€‚"

        insights.append(sop_text)

        return jsonify({"status": "success", "data": insights})
    except Exception as e:
        return jsonify({"status": "error", "message": f"ç®—æ³•è¿ç®—å¼‚å¸¸: {str(e)}"}), 500


def do_predict():
    """ã€é¢„æµ‹æ¨¡å—ä¸€ï¼šåŸºäºéšæœºæ£®æ— (Random Forest) çš„ç‰¹å¾é‡è¦æ€§æ‹†è§£ã€‘"""
    try:
        data = request.json
        filename = data.get('filename')
        target_col = data.get('target_col')
        feature_cols = data.get('feature_cols', [])

        if not filename or not target_col or len(feature_cols) == 0:
            return jsonify({"status": "error", "message": "å‚æ•°è®¾å®šä¸å®Œæ•´"}), 400

        df = read_df(os.path.join(UPLOAD_FOLDER, filename))
        df_clean = df[[target_col] + feature_cols].dropna()

        if len(df_clean) < 10:
            return jsonify({"status": "error", "message": "å»é™¤ç¼ºå¤±å€¼åæœ‰æ•ˆæ•°æ®é‡è¿‡ä½ï¼Œæ¨¡å‹æ— æ³•æ”¶æ•›"}), 400

        X = df_clean[feature_cols]
        y = df_clean[target_col]

        # ä»¥ 8:2 åˆ‡åˆ†æ„å»ºè®­ç»ƒé›†ä¸æµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # å®ä¾‹åŒ– Bagging é›†æˆæ ‘æ¨¡å‹ï¼Œé”å®šéšæœºç§å­ä»¥ä¿è¯ç»“æœå¤ç°æ€§
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        r2 = r2_score(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)

        importances = model.feature_importances_.tolist()
        feature_importances = [round(i * 100, 2) for i in importances]

        # é™ç»´æŠ½æ ·ï¼Œä¸‹å‘ä¸è¶…è¿‡ 100 æ¡çœŸå®/é¢„æµ‹å¯¹ç»™å‰ç«¯æ•£ç‚¹å›¾
        scatter_data = [[float(actual), float(pred)] for actual, pred in
                        zip(y_test.tolist()[:100], y_pred.tolist()[:100])]

        return jsonify({
            "status": "success",
            "data": {
                "r2": round(r2, 4),
                "mse": round(mse, 4),
                "features": feature_cols,
                "importances": feature_importances,
                "scatter": scatter_data
            }
        })
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


def do_predict_new():
    """ã€é¢„æµ‹æ¨¡å—äºŒï¼šé›†æˆæ—¶é—´åºåˆ—å¹³æ»‘ä¸ç½®ä¿¡åº¦åŠ¨æ€æƒ©ç½šè§„åˆ™çš„æ¨ç†å¼•æ“ã€‘"""
    try:
        data = request.json
        filename = data.get('filename')
        target_col = data.get('target_col')
        feature_cols = data.get('feature_cols', [])

        if not filename or not target_col or len(feature_cols) == 0:
            return jsonify({"status": "error", "message": "å‚æ•°è®¾å®šä¸å®Œæ•´"}), 400

        df = read_df(os.path.join(UPLOAD_FOLDER, filename))
        df_clean = df[[target_col] + feature_cols].dropna()

        if len(df_clean) < 10:
            return jsonify({"status": "error", "message": "æœ‰æ•ˆæ•°æ®é‡ä¸è¶³"}), 400

        X = df_clean[feature_cols]
        y = df_clean[target_col]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)

        # æå– 50 æ¡è¿ç»­åºåˆ—è¿›è¡Œä»¿çœŸæ¨ç†å±•ç¤º
        sample_size = min(50, len(X_test))
        X_show = X_test.iloc[:sample_size]
        y_show_real = y_test.iloc[:sample_size].values
        y_show_pred = model.predict(X_show)

        # ====== ğŸš€ æ ¸å¿ƒç®—æ³•åˆ›æ–°ï¼šå¼•å…¥å¯å‘å¼å¤šç»´æƒ©ç½šæœºåˆ¶ ======
        sample_r2 = r2_score(y_show_real, y_show_pred)
        std_pred = np.std(y_show_pred)
        std_real = np.std(y_show_real)

        # 1. Pearson è¶‹åŠ¿ç›¸å…³ç³»æ•°ï¼ˆä¾¦æµ‹æ‹Ÿåˆæ›²çº¿ç›¸ä½æ˜¯å¦ä¸€è‡´ï¼‰
        if std_pred == 0 or std_real == 0:
            corr = 0
        else:
            corr = np.corrcoef(y_show_real, y_show_pred)[0, 1]

        # 2. å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE) æ¨å¯¼åŸºç¡€å‡†ç¡®ç‡
        mape = np.mean(np.abs((y_show_real - y_show_pred) / (y_show_real + 1e-9)))
        acc = max(0, 1 - mape)

        # 3. é€€åŒ–ç†”æ–­ï¼šè‹¥ R2 æä½æˆ–ç›¸å…³æ€§ç ´ç­ï¼Œä»£è¡¨æ¨¡å‹é€€åŒ–ä¸ºçº¯å‡å€¼ç›´çº¿ã€‚æ­¤æ—¶è§¦å‘æƒ©ç½šç³»æ•° 0.45
        if sample_r2 < 0.15 or corr < 0.3:
            confidence = round(acc * 45, 2)
        else:
            confidence = round((acc * 0.4 + corr * 0.6) * 100, 2)

        if confidence > 99:
            confidence = 98.75

        labels = [f"æµ‹è¯•ç‚¹ {i + 1}" for i in range(sample_size)]

        return jsonify({
            "status": "success",
            "data": {
                "confidence": confidence,
                "sampleSize": sample_size,
                "labels": labels,
                "realValues": np.round(y_show_real, 2).tolist(),
                "predictedValues": np.round(y_show_pred, 2).tolist()
            }
        })
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500