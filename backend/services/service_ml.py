import os
import pandas as pd
import numpy as np
from flask import request, jsonify
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from config import UPLOAD_FOLDER
from utils import read_df


def do_summary():
    try:
        filename = request.json.get('filename')
        if not filename: return jsonify({"status": "error", "message": "ç³»ç»Ÿæœªæ‰¾åˆ°å½“å‰å¤„ç†çš„æ–‡ä»¶"}), 400
        df = read_df(os.path.join(UPLOAD_FOLDER, filename))

        # å°è¯•å°†èƒ½è½¬æ•°å€¼çš„åˆ—éƒ½è½¬æˆæ•°å€¼
        for col in df.columns:
            try:
                df[col] = pd.to_numeric(df[col])
            except Exception:
                pass

        # æ ¸å¿ƒåˆ†ç±»ï¼šè¯†åˆ«é‡åŒ–æŒ‡æ ‡ä¸ç±»åˆ«æŒ‡æ ‡
        all_cols = df.columns.tolist()
        numeric_cols = [c for c in all_cols if pd.api.types.is_numeric_dtype(df[c]) and not any(
            kw in str(c).lower() for kw in ['å·', 'id', 'ç¼–å·', 'ä»£ç '])]
        categorical_cols = [c for c in all_cols if c not in numeric_cols]

        # æå–ç‰¹æ®Šç‰¹å¾ç”¨äºæŒ‡å¯¼
        binary_cols = [c for c in all_cols if df[c].nunique() == 2]

        row_count = len(df)
        col_count = len(df.columns)
        insights = []

        # ==========================================
        # ğŸš€ 1. æ–°å¢ï¼šæ•°æ®è´¨é‡ä¸å¯ç”¨æ€§æƒå¨è¯„ä¼°
        # ==========================================
        total_cells = row_count * col_count
        missing_cells = df.isnull().sum().sum()
        missing_rate = missing_cells / total_cells if total_cells > 0 else 0
        quality_score = 100 - (missing_rate * 100)

        if row_count < 30:
            quality_desc = f"âš ï¸ **æ•°æ®è´¨é‡è¯„ä¼°**ï¼šå½“å‰æ•°æ®é›†è§„æ¨¡æå°ï¼ˆä»… {row_count} æ¡ï¼‰ã€‚è™½ç„¶æ•°æ®å®Œæ•´åº¦ä¸º {quality_score:.1f}%ï¼Œä½†åœ¨è¿›è¡Œå¤æ‚æœºå™¨å­¦ä¹ æ—¶ææ˜“å‡ºç°â€œè¿‡æ‹Ÿåˆâ€ã€‚å»ºè®®ä¼˜å…ˆä½¿ç”¨åŸºç¡€çš„ã€Œæè¿°ç»Ÿè®¡ã€å’Œã€Œt æ£€éªŒã€ã€‚"
        elif missing_rate > 0.15:
            quality_desc = f"âš ï¸ **æ•°æ®è´¨é‡è¯„ä¼°**ï¼šå½“å‰æ•°æ®ç¼ºå¤±ç‡è¾ƒé«˜ï¼ˆç©ºå€¼å æ¯” {missing_rate * 100:.1f}%ï¼‰ã€‚å»ºè®®æ‚¨å…ˆåœ¨å·¦ä¾§æ§åˆ¶å°ç‚¹å‡»ã€âœ¨ æ™ºèƒ½æ¸…æ´—ã€‘è¿›è¡Œç¼ºå¤±å€¼æ’è¡¥ï¼Œå¦åˆ™å°†ä¸¥é‡å½±å“åç»­æœºå™¨å­¦ä¹ é¢„æµ‹çš„å‡†ç¡®æ€§ï¼"
        else:
            quality_desc = f"ğŸ›¡ï¸ **æ•°æ®è´¨é‡è¯„ä¼°**ï¼šæä½³ï¼å½“å‰æ•°æ®é›†åŒ…å« {row_count} æ¡æ ·æœ¬ï¼Œæ•°æ®å®Œæ•´åº¦é«˜è¾¾ {quality_score:.1f}%ã€‚ç‰¹å¾çŸ©é˜µååˆ†å¥åº·ï¼Œå®Œå…¨å…·å¤‡è¿›å…¥æ·±å±‚æœºå™¨å­¦ä¹ ä¸å…³è”æŒ–æ˜çš„åˆ†ææ ‡å‡†ã€‚"
        insights.append(quality_desc)

        insights.append(
            f"ğŸ“¦ **å…¨å±€ç»“æ„å‰–æ**ï¼šç³»ç»ŸæˆåŠŸè§£æå‡º **{len(numeric_cols)}** ä¸ªã€Œé‡åŒ–æ•°å€¼æŒ‡æ ‡ã€ä¸ **{len(categorical_cols)}** ä¸ªã€Œå®šæ€§ç±»åˆ«æŒ‡æ ‡ã€ã€‚")

        # ==========================================
        # ğŸš€ 2. æ–°å¢ï¼šæç«¯åæ€(æ–­æ¡£)åˆ†å¸ƒä¸å¼‚å¸¸å€¼ä¾¦æµ‹
        # ==========================================
        if len(numeric_cols) > 0:
            skew_insights = []
            for col in numeric_cols:
                skew_val = df[col].skew()
                if pd.notna(skew_val):
                    # ååº¦ç»å¯¹å€¼å¤§äº 1.5 è§†ä¸ºå­˜åœ¨ä¸¥é‡çš„é•¿å°¾æ–­æ¡£
                    if skew_val > 1.5:
                        skew_insights.append(f"ã€{col}ã€‘å­˜åœ¨ä¸¥é‡çš„â€œå³åâ€æ–­æ¡£ï¼ˆå³æå°‘æ•°ä¸ªä½“æ•°å€¼å¼‚å¸¸åé«˜ï¼Œæ‹‰é«˜äº†å¹³å‡æ°´ä½ï¼‰")
                    elif skew_val < -1.5:
                        skew_insights.append(f"ã€{col}ã€‘å­˜åœ¨ä¸¥é‡çš„â€œå·¦åâ€æ–­æ¡£ï¼ˆå³æå°‘æ•°ä¸ªä½“æ•°å€¼å¼‚å¸¸åä½ï¼Œä¸¥é‡æ‹–äº†æ•´ä½“åè…¿ï¼‰")

            if skew_insights:
                insights.append("ğŸ“‰ **æç«¯åˆ†å¸ƒé¢„è­¦**ï¼šç³»ç»Ÿä¾¦æµ‹åˆ°éƒ¨åˆ†æŒ‡æ ‡å­˜åœ¨æ–­æ¡£å¼åˆ†å¸ƒã€‚å…¶ä¸­ï¼š" + "ï¼›".join(
                    skew_insights) + "ã€‚å»ºè®®åœ¨å»ºæ¨¡å‰ä½¿ç”¨ã€âœ¨ æ™ºèƒ½æ¸…æ´—ã€‘å¤„ç†æç«¯å¼‚å¸¸å€¼ï¼ˆåŸºäº 3Ïƒ åŸåˆ™ï¼‰ï¼Œæˆ–é€šè¿‡ã€Œå¯è§†åŒ–ã€æŸ¥çœ‹å…¶ç›´æ–¹å›¾è¡¨ç°ã€‚")
            else:
                insights.append(
                    "ğŸ“Š **æ•°æ®åˆ†å¸ƒå¥åº·åº¦**ï¼šå„æ•°å€¼å‹æŒ‡æ ‡åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€ï¼Œæœªæ£€æµ‹åˆ°æå…¶ä¸¥é‡çš„å•è¾¹â€œæ–­æ¡£â€æˆ–æç«¯å¼‚å¸¸åæ€åˆ†å¸ƒã€‚")

        # ==========================================
        # 3. tæ£€éªŒé¶å‘æ¨è
        # ==========================================
        if binary_cols:
            insights.append(
                f"âš–ï¸ **ç¾¤ä½“å·®å¼‚æ€§æ£€éªŒæ¨è**ï¼šç³»ç»Ÿä¾¦æµ‹åˆ°å®Œç¾çš„äºŒåˆ†ç±»å˜é‡ã€{binary_cols[0]}ã€‘ã€‚å¼ºçƒˆå»ºè®®æ‚¨å°†å…¶è®¾ä¸º**ã€Œt æ£€éªŒåˆ†ç»„å˜é‡ã€**ï¼Œå»éªŒè¯ä¸åŒç¾¤ä½“åœ¨è¯¥ç»´åº¦ä¸‹æ˜¯å¦å­˜åœ¨ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼")

        # ==========================================
        # 4. æœºå™¨å­¦ä¹ é¶ç‚¹æ¨è (å¸¦æ¡ä»¶åˆ¤å®š)
        # ==========================================
        has_ai_recommendation = False
        if len(numeric_cols) >= 3:
            corr_df = df[numeric_cols].corr().abs()
            target_candidate = corr_df.sum().idxmax()
            feature_candidates = [c for c in numeric_cols if c != target_candidate]

            insights.append(
                f"ğŸ¤– **AI é¢„æµ‹å¼•æ“æœ€ä½³ç»„åˆ**ï¼šç»è¿‡ç‰¹å¾çŸ©é˜µæ‰«æï¼Œã€{target_candidate}ã€‘ä¸å…¶ä»–å˜é‡å­˜åœ¨æœ€å¹¿æ³›çš„è”åŠ¨æ•ˆåº”ã€‚å¼ºçƒˆå»ºè®®å°†ã€{target_candidate}ã€‘è®¾ä¸º **ç›®æ ‡å˜é‡ (Y)**ï¼Œå°†ã€{', '.join(feature_candidates[:3])} ç­‰ã€‘è®¾ä¸º **ç‰¹å¾å˜é‡ (X)**ï¼Œä¸€é”®è®­ç»ƒé¢„æµ‹æ¨¡å‹ï¼")
            has_ai_recommendation = True

        # ==========================================
        # ğŸš€ 5. ä¿å§†çº§ SOP å¯¼èˆª (ä¿®å¤é€»è¾‘ Bug)
        # ==========================================
        sop_text = "ğŸ—ºï¸ **é›¶åŸºç¡€ä¸“å±Â·æç®€åˆ†æå¯¼èˆª (SOP)**ï¼š<br>ğŸ‘‰ **Step 1 æ¢åº•**ï¼šç‚¹å‡»ã€Œæè¿°ç»Ÿè®¡ã€ä¸ã€Œå¯è§†åŒ–ã€ï¼Œçœ‹ä¸€çœ¼æ•°æ®çš„å‡å€¼è¡¨ç°ä¸å›¾è¡¨åˆ†å¸ƒè§„å¾‹ã€‚<br>ğŸ‘‰ **Step 2 å¯»å› **ï¼šç‚¹å‡»ã€Œå…³è”åˆ†æã€ï¼Œçœ‹ä¸€çœ¼çƒ­åŠ›å›¾ï¼Œé¢œè‰²è¶Šæ·±è¯´æ˜ä¸¤ä¸ªæŒ‡æ ‡â€œæ†ç»‘â€å¾—è¶Šç´§ã€‚"

        # åŠ¨æ€ç»„è£… Step 3ï¼šå¦‚æœæœ‰æ¨èç»„åˆå°±å¼•æµï¼Œæ²¡æœ‰å°±é€šç”¨å¼•å¯¼
        if has_ai_recommendation:
            sop_text += "<br>ğŸ‘‰ **Step 3 é¢„æµ‹**ï¼šä¸¥æ ¼ç…§æŠ„ä¸Šæ–¹ã€AI é¢„æµ‹å¼•æ“æœ€ä½³ç»„åˆã€‘ï¼Œè¿›å…¥ã€Œæœºå™¨å­¦ä¹ å¼•æ“ã€ç‚¹å‡»è®­ç»ƒï¼Œäº«å— AI è‡ªåŠ¨æ¨ç†ï¼"
        else:
            sop_text += "<br>ğŸ‘‰ **Step 3 æ¢ç´¢**ï¼šè¿›å…¥å·¦ä¾§ã€Œæœºå™¨å­¦ä¹ å¼•æ“ã€ï¼Œè‡ªç”±å‹¾é€‰æ‚¨æ„Ÿå…´è¶£çš„ç›®æ ‡å˜é‡ä¸ç‰¹å¾å˜é‡ï¼Œå°è¯•æŒ–æ˜æ½œåœ¨çš„æ•°æ®è§„å¾‹ã€‚"

        insights.append(sop_text)

        return jsonify({"status": "success", "data": insights})
    except Exception as e:
        return jsonify({"status": "error", "message": f"ç®—æ³•è¿ç®—å¼‚å¸¸: {str(e)}"}), 500





def do_predict():
    try:
        data = request.json
        filename, target_col, feature_cols = data.get('filename'), data.get('target_col'), data.get('feature_cols', [])
        if not filename or not target_col or len(feature_cols) == 0: return jsonify(
            {"status": "error", "message": "å‚æ•°ä¸å®Œæ•´"}), 400

        df = read_df(os.path.join(UPLOAD_FOLDER, filename))
        df_clean = df[[target_col] + feature_cols].dropna()
        if len(df_clean) < 10: return jsonify({"status": "error", "message": "æœ‰æ•ˆæ•°æ®é‡å¤ªå°‘ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹"}), 400

        X, y = df_clean[feature_cols], df_clean[target_col]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        r2, mse = r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred)
        scatter_data = [[float(actual), float(pred)] for actual, pred in
                        zip(y_test.tolist()[:100], y_pred.tolist()[:100])]

        return jsonify({"status": "success",
                        "data": {"r2": round(r2, 4), "mse": round(mse, 4), "features": feature_cols,
                                 "importances": [round(i * 100, 2) for i in model.feature_importances_.tolist()],
                                 "scatter": scatter_data}})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


def do_predict_new():
    """V1.2 ç®—æ³•è¿›åŒ–ç‰ˆï¼šå¼•å…¥ R2 ä¸è¶‹åŠ¿æƒ©ç½šçš„æ™ºèƒ½æ¨ç†æ¥å£"""
    try:
        data = request.json
        filename, target_col, feature_cols = data.get('filename'), data.get('target_col'), data.get('feature_cols', [])
        if not filename or not target_col or len(feature_cols) == 0:
            return jsonify({"status": "error", "message": "å‚æ•°ä¸å®Œæ•´"}), 400

        df = read_df(os.path.join(UPLOAD_FOLDER, filename))
        df_clean = df[[target_col] + feature_cols].dropna()
        if len(df_clean) < 10:
            return jsonify({"status": "error", "message": "æœ‰æ•ˆæ•°æ®é‡å¤ªå°‘"}), 400

        X, y = df_clean[feature_cols], df_clean[target_col]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)

        sample_size = min(50, len(X_test))
        X_show = X_test.iloc[:sample_size]
        y_show_real = y_test.iloc[:sample_size].values
        y_show_pred = model.predict(X_show)

        # ====== ğŸš€ æ ¸å¿ƒç®—æ³•ä¼˜åŒ–ï¼šå¼•å…¥å¤šç»´è¯„ä¼°ä¸æƒ©ç½šæœºåˆ¶ ======
        sample_r2 = r2_score(y_show_real, y_show_pred)
        std_pred = np.std(y_show_pred)
        std_real = np.std(y_show_real)

        # 1. è®¡ç®— Pearson è¶‹åŠ¿ç›¸å…³ç³»æ•° (çœ‹ä¸¤æ¡çº¿çš„æ³¢åŠ¨æ˜¯å¦ä¸€è‡´)
        if std_pred == 0 or std_real == 0:
            corr = 0
        else:
            corr = np.corrcoef(y_show_real, y_show_pred)[0, 1]

        # 2. è®¡ç®—åŸºç¡€æ•°å€¼å‡†ç¡®åº¦
        mape = np.mean(np.abs((y_show_real - y_show_pred) / (y_show_real + 1e-9)))
        acc = max(0, 1 - mape)

        # 3. ç»ˆæå®¡åˆ¤é€»è¾‘ï¼šå¦‚æœæ¨¡å‹é€€åŒ–æˆç›´çº¿(R2æä½æˆ–æ— ç›¸å…³æ€§)ï¼Œè¿›è¡Œæ®‹é…·æƒ©ç½šï¼
        if sample_r2 < 0.15 or corr < 0.3:
            # è§¦å‘æƒ©ç½šï¼šä¸ç®¡æ•°å€¼å¤šæ¥è¿‘ï¼Œæœ€é«˜ä¸èƒ½è¶…è¿‡ 45% çš„ç½®ä¿¡åº¦
            confidence = round(acc * 45, 2)
        else:
            # æ­£å¸¸è¡¨ç°ï¼šç»¼åˆè€ƒè™‘å‡†ç¡®ç‡(40%)å’Œæ³¢åŠ¨è¶‹åŠ¿(60%)
            confidence = round((acc * 0.4 + corr * 0.6) * 100, 2)

        if confidence > 99: confidence = 98.75

        return jsonify({
            "status": "success",
            "data": {
                "confidence": confidence,
                "sampleSize": sample_size,
                "labels": [f"æµ‹è¯•ç‚¹ {i+1}" for i in range(sample_size)],
                "realValues": np.round(y_show_real, 2).tolist(),
                "predictedValues": np.round(y_show_pred, 2).tolist()
            }
        })
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500