import os
import pandas as pd
import numpy as np
from flask import request, jsonify
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from config import UPLOAD_FOLDER
from utils import read_df


def do_summary():
    try:
        filename = request.json.get('filename')
        if not filename: return jsonify({"status": "error", "message": "ç³»ç»Ÿæœªæ‰¾åˆ°å½“å‰å¤„ç†çš„æ–‡ä»¶"}), 400
        df = read_df(os.path.join(UPLOAD_FOLDER, filename))

        for col in df.columns:
            try:
                df[col] = pd.to_numeric(df[col])
            except Exception:
                pass

        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and not any(
            kw in str(c).lower() for kw in ['å·', 'id', 'ç¼–å·', 'ä»£ç '])]
        insights = [f"âœ¨ **æ•°æ®æ¦‚è§ˆ**ï¼šå½“å‰æ•°æ®é›†å…±åŒ…å« {len(df)} æ¡è®°å½•ï¼ŒæˆåŠŸè¯†åˆ«å‡º {len(numeric_cols)} ä¸ªå…³é”®æ•°å€¼æŒ‡æ ‡ã€‚"]

        if len(numeric_cols) > 0:
            std_series = df[numeric_cols].std()
            if not std_series.isna().all():
                insights.append(
                    f"ğŸ“Š **æ³¢åŠ¨æ´å¯Ÿ**ï¼šæ•°æ®ä¸­ã€{std_series.idxmax()}ã€‘çš„ç¦»æ•£ç¨‹åº¦æœ€å¤§ï¼Œè¯´æ˜ä¸ªä½“å·®å¼‚æœ€æ˜¾è‘—ï¼›è€Œã€{std_series.idxmin()}ã€‘åˆ†å¸ƒæœ€ä¸ºé›†ä¸­ç¨³å®šã€‚")

            if len(numeric_cols) >= 2:
                corr_df = df[numeric_cols].corr().abs()
                for i in range(len(corr_df.columns)): corr_df.iloc[i, i] = 0.0
                if not corr_df.isna().all().all():
                    max_corr = corr_df.max().max()
                    if pd.notna(max_corr) and max_corr > 0.5:
                        row_idx, col_idx = np.unravel_index(np.nanargmax(corr_df.to_numpy()), corr_df.shape)
                        insights.append(
                            f"ğŸ”— **å¼ºå…³è”å‘ç°**ï¼šç³»ç»Ÿæ£€æµ‹åˆ°ã€{corr_df.index[row_idx]}ã€‘ä¸ã€{corr_df.columns[col_idx]}ã€‘å­˜åœ¨è¾ƒå¼ºçš„çº¿æ€§ç›¸å…³æ€§ï¼ˆr={round(max_corr, 2)}ï¼‰ã€‚")

        insights.append("ğŸ’¡ **åˆ†æå»ºè®®**ï¼šå»ºè®®ç»“åˆå³ä¾§çš„é›·è¾¾å›¾è¿›è¡Œä¸ªä½“ä¼˜åŠ¿è¯Šæ–­ï¼Œå¹¶åˆ©ç”¨ t æ£€éªŒè¿›ä¸€æ­¥æ¢ç´¢ä¸åŒç¾¤ä½“é—´çš„å·®å¼‚ã€‚")
        return jsonify({"status": "success", "data": insights})
    except Exception as e:
        return jsonify({"status": "error", "message": f"ç®—æ³•è¿ç®—å¼‚å¸¸: {str(e)}"}), 500


def do_predict():
    try:
        data = request.json
        filename, target_col, feature_cols = data.get('filename'), data.get('target_col'), data.get('feature_cols', [])
        if not filename or not target_col or len(feature_cols) == 0: return jsonify(
            {"status": "error", "message": "å‚æ•°ä¸å®Œæ•´"}), 400

        df = read_df(os.path.join(UPLOAD_FOLDER, filename))
        df_clean = df[[target_col] + feature_cols].dropna()
        if len(df_clean) < 10: return jsonify({"status": "error", "message": "æœ‰æ•ˆæ•°æ®é‡å¤ªå°‘ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹"}), 400

        X, y = df_clean[feature_cols], df_clean[target_col]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        r2, mse = r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred)
        scatter_data = [[float(actual), float(pred)] for actual, pred in
                        zip(y_test.tolist()[:100], y_pred.tolist()[:100])]

        return jsonify({"status": "success",
                        "data": {"r2": round(r2, 4), "mse": round(mse, 4), "features": feature_cols,
                                 "importances": [round(i * 100, 2) for i in model.feature_importances_.tolist()],
                                 "scatter": scatter_data}})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500