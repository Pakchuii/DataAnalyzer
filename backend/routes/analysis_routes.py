import os
import pandas as pd
import numpy as np
import scipy.stats as stats
from flask import Blueprint, request, jsonify
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

from config import UPLOAD_FOLDER

analysis_bp = Blueprint('analysis', __name__)


def read_df(filepath):
    """
    é€šç”¨æ•°æ®è¯»å–å·¥å…·å‡½æ•°
    æ”¯æŒè‡ªåŠ¨è¯†åˆ« csv ä¸ excel æ ¼å¼ï¼Œå¹¶å¤„ç†å¸¸è§ç¼–ç é—®é¢˜
    """
    ext = filepath.rsplit('.', 1)[1].lower()
    if ext == 'csv':
        try:
            return pd.read_csv(filepath, encoding='utf-8')
        except UnicodeDecodeError:
            return pd.read_csv(filepath, encoding='gbk')
    else:
        return pd.read_excel(filepath, engine='openpyxl' if ext == 'xlsx' else 'xlrd')


# ==========================================
# æ¨¡å—ä¸€ï¼šåŸºç¡€ç»Ÿè®¡ä¸åˆ†å¸ƒå¯è§†åŒ–
# ==========================================

@analysis_bp.route('/api/analyze/descriptive', methods=['POST'])
def descriptive_analysis():
    """æ‰§è¡Œæè¿°æ€§ç»Ÿè®¡åˆ†æï¼ˆå‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ç­‰ï¼‰"""
    try:
        df = read_df(os.path.join(UPLOAD_FOLDER, request.json.get('filename')))
        selected = [c for c in request.json.get('columns', []) if c in df.columns]

        stats_data = []
        for col in selected:
            d = df[col].dropna()
            stats_data.append({
                "variable": str(col),
                "count": int(d.count()),
                "mean": float(round(d.mean(), 4)),
                "median": float(round(d.median(), 4)),
                "std": float(round(d.std(), 4)),
                "min": float(round(d.min(), 4)),
                "max": float(round(d.max(), 4))
            })

        return jsonify({"status": "success", "data": stats_data})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


@analysis_bp.route('/api/visualize/distribution', methods=['POST'])
def visualize_distribution():
    """ç”Ÿæˆæ•°å€¼å‹å˜é‡çš„ç›´æ–¹å›¾ä¸ç®±çº¿å›¾æ•°æ®"""
    try:
        df = read_df(os.path.join(UPLOAD_FOLDER, request.json.get('filename')))
        selected = request.json.get('columns', [])

        charts_data = []
        for col in selected:
            if col not in df.columns:
                continue

            d = df[col].dropna()
            if len(d) == 0:
                continue

            counts, bin_edges = np.histogram(d, bins='auto')

            charts_data.append({
                "variable": col,
                "boxplot": [
                    float(d.min()), float(d.quantile(0.25)), float(d.median()),
                    float(d.quantile(0.75)), float(d.max())
                ],
                "histogram": {
                    "categories": [f"{round(bin_edges[i], 1)}~{round(bin_edges[i + 1], 1)}" for i in
                                   range(len(counts))],
                    "series": [int(c) for c in counts]
                }
            })

        return jsonify({"status": "success", "data": charts_data})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


@analysis_bp.route('/api/visualize/categorical', methods=['POST'])
def visualize_categorical():
    """ç”Ÿæˆåˆ†ç±»å˜é‡çš„é¥¼å›¾æ•°æ®"""
    try:
        df = read_df(os.path.join(UPLOAD_FOLDER, request.json.get('filename')))
        cat_cols = df.select_dtypes(include=['object']).columns.tolist()

        charts_data = []
        for col in cat_cols:
            # ä¸¥æ ¼è¿‡æ»¤ï¼šç»å¯¹ä¸è®©å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆå¦‚å§“åã€å­¦å·ï¼‰å»ç”»é¥¼å›¾
            if any(kw in str(col).lower() for kw in ['å§“å', 'name', 'å·', 'id']) or df[col].nunique() > 15:
                continue

            counts = df[col].value_counts()
            charts_data.append({
                "variable": col,
                "categories": counts.index.tolist(),
                "values": counts.values.tolist(),
                "pie_data": [{"name": str(k), "value": int(v)} for k, v in counts.items()]
            })

        return jsonify({"status": "success", "data": charts_data})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


# ==========================================
# æ¨¡å—äºŒï¼šé«˜é˜¶åˆ†æï¼ˆç›¸å…³æ€§ã€tæ£€éªŒã€é›·è¾¾å›¾ï¼‰
# ==========================================

@analysis_bp.route('/api/get_options', methods=['POST'])
def get_options():
    """è·å–æŒ‡å®šåˆ—çš„æ‰€æœ‰å”¯ä¸€é€‰é¡¹ï¼ˆç”¨äºé›·è¾¾å›¾ä¸‹æ‹‰æ¡†ï¼‰"""
    try:
        df = read_df(os.path.join(UPLOAD_FOLDER, request.json.get('filename')))
        col = request.json.get('column')

        if col not in df.columns:
            return jsonify({"status": "error", "message": "åˆ—ä¸å­˜åœ¨"}), 400

        # å¼ºåˆ¶å°†è¯¥åˆ—è½¬ä¸ºå­—ç¬¦ä¸²å¹¶å»é‡ï¼Œæœ€å¤šå– 1000 ä¸ªé˜²æ­¢æµè§ˆå™¨å¡æ­»
        options = df[col].dropna().astype(str).unique().tolist()[:1000]
        return jsonify({"status": "success", "data": options})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


@analysis_bp.route('/api/analyze/advanced', methods=['POST'])
def advanced_analysis():
    """é«˜é˜¶å…³è”åˆ†æï¼šæ­£æ€æ€§æ£€éªŒã€çš®å°”é€Šç›¸å…³ç³»æ•°çŸ©é˜µã€æ•£ç‚¹å›¾"""
    try:
        df = read_df(os.path.join(UPLOAD_FOLDER, request.json.get('filename')))
        selected = [c for c in request.json.get('columns', []) if c in df.columns]

        if len(selected) < 2:
            return jsonify({"status": "error", "message": "å˜é‡è¿‡å°‘ï¼Œæ— æ³•è¿›è¡Œå…³è”åˆ†æ"}), 400

        # 1. æ­£æ€æ€§æ£€éªŒ (Shapiro-Wilk)
        normality_results = []
        for col in selected:
            d = df[col].dropna()
            if len(d) >= 3:
                stat, p = stats.shapiro(d)
                normality_results.append({
                    "variable": col,
                    "statistic": float(round(stat, 4)),
                    "p_value": float(round(p, 4)),
                    "is_normal": bool(p > 0.05)
                })

        # 2. ç›¸å…³æ€§çŸ©é˜µæ„å»º
        corr_matrix = []
        corr_df = df[selected].corr(method='pearson').fillna(0).round(3)
        for i in range(len(selected)):
            for j in range(len(selected)):
                corr_matrix.append([i, j, float(corr_df.iloc[i, j])])

        # 3. æ•£ç‚¹å›¾æ•°æ®æŠ½å– (å–å‰ä¸¤ä¸ªå˜é‡)
        var1, var2 = selected[0], selected[1]
        scatter_vars = [var1, var2]
        temp_df = df[[var1, var2]].copy().dropna()
        scatter_data = temp_df.values.tolist()

        return jsonify({
            "status": "success",
            "data": {
                "variables": selected,
                "normality": normality_results,
                "correlation_matrix": corr_matrix,
                "scatter_data": scatter_data,
                "scatter_vars": scatter_vars
            }
        })
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


@analysis_bp.route('/api/analyze/ttest', methods=['POST'])
def ttest_analysis():
    """ç‹¬ç«‹æ ·æœ¬ t æ£€éªŒ"""
    try:
        df = read_df(os.path.join(UPLOAD_FOLDER, request.json.get('filename')))
        group_col = request.json.get('group_col')
        target_cols = request.json.get('columns', [])

        if not group_col or group_col not in df.columns:
            return jsonify({"status": "error", "message": "æœªé€‰æ‹©æœ‰æ•ˆåˆ†ç»„å˜é‡"}), 400

        groups = df[group_col].dropna().unique()
        if len(groups) != 2:
            return jsonify({"status": "error", "message": "éäºŒåˆ†ç±»å˜é‡ï¼Œæ— æ³•è¿›è¡Œ t æ£€éªŒ"}), 400

        group1_data = df[df[group_col] == groups[0]]
        group2_data = df[df[group_col] == groups[1]]
        results = []

        for col in target_cols:
            if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
                d1 = group1_data[col].dropna()
                d2 = group2_data[col].dropna()

                if len(d1) > 1 and len(d2) > 1:
                    stat, p = stats.ttest_ind(d1, d2, equal_var=False)
                    results.append({
                        "variable": col,
                        "group1_name": str(groups[0]),
                        "group1_mean": float(round(d1.mean(), 4)),
                        "group2_name": str(groups[1]),
                        "group2_mean": float(round(d2.mean(), 4)),
                        "t_value": float(round(stat, 4)),
                        "p_value": float(round(p, 4)),
                        "significant": bool(p < 0.05)
                    })

        return jsonify({"status": "success", "data": results})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


@analysis_bp.route('/api/visualize/radar', methods=['POST'])
def visualize_radar():
    """ä¸ªä½“é›·è¾¾å›¾èƒ½åŠ›è¯Šæ–­"""
    try:
        filename = request.json.get('filename')
        id_col = request.json.get('id_col')
        target_val = request.json.get('target_val')

        df = read_df(os.path.join(UPLOAD_FOLDER, filename))
        numeric_df = df.select_dtypes(include=['number'])

        # è¿‡æ»¤æ— æ•ˆæŒ‡æ ‡
        valid_numeric_cols = [c for c in numeric_df.columns if
                              not any(kw in str(c).lower() for kw in ['å·', 'id', 'ç¼–å·', 'ä»£ç '])]
        if not valid_numeric_cols:
            return jsonify({"status": "error", "message": "æœªæ‰¾åˆ°å¯ç”¨äºé›·è¾¾å›¾çš„æ•°å€¼æŒ‡æ ‡"}), 400

        avg_data = numeric_df[valid_numeric_cols].mean().round(2).tolist()

        # å¼ºåˆ¶ç±»å‹è½¬æ¢æ¯”å¯¹ï¼Œè§£å†³å­¦å·ç­‰å­—ç¬¦ä¸²æ•°å­—åŒ¹é…é—®é¢˜
        target_row = df[df[id_col].astype(str) == str(target_val)]
        if target_row.empty:
            return jsonify({"status": "error", "message": "æœªæ‰¾åˆ°æŒ‡å®šä¸ªä½“"}), 400

        target_data = target_row[valid_numeric_cols].iloc[0].fillna(0).round(2).tolist()
        indicators = [{"name": col, "max": float(numeric_df[col].max()) * 1.1} for col in valid_numeric_cols]

        return jsonify({
            "status": "success",
            "data": {
                "indicators": indicators,
                "avg_data": avg_data,
                "target_data": target_data,
                "target_name": str(target_val)
            }
        })
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


# ==========================================
# æ¨¡å—ä¸‰ï¼šæ™ºèƒ½åŒ–ä¸æœºå™¨å­¦ä¹ ç®—æ³•
# ==========================================

@analysis_bp.route('/api/analyze/summary', methods=['POST'])
def generate_summary():
    """AI æ™ºèƒ½æ–‡å­—æ•°æ®è§£è¯»æŠ¥å‘Šç”Ÿæˆ"""
    try:
        filename = request.json.get('filename')
        if not filename:
            return jsonify({"status": "error", "message": "ç³»ç»Ÿæœªæ‰¾åˆ°å½“å‰å¤„ç†çš„æ–‡ä»¶"}), 400

        df = read_df(os.path.join(UPLOAD_FOLDER, filename))

        # å°è¯•å°†æ‰€æœ‰åˆ—è½¬ä¸ºæ•°å€¼å‹ï¼Œä¾¿äºåç»­æå–
        for col in df.columns:
            try:
                df[col] = pd.to_numeric(df[col])
            except Exception:
                pass

        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and not any(
            kw in str(c).lower() for kw in ['å·', 'id', 'ç¼–å·', 'ä»£ç '])]
        insights = []

        # 1. æ¦‚è§ˆæ´å¯Ÿ
        insights.append(
            f"âœ¨ **æ•°æ®æ¦‚è§ˆ**ï¼šå½“å‰æ•°æ®é›†å…±åŒ…å« {len(df)} æ¡è®°å½•ï¼ŒæˆåŠŸè¯†åˆ«å‡º {len(numeric_cols)} ä¸ªå…³é”®æ•°å€¼æŒ‡æ ‡ã€‚")

        if len(numeric_cols) > 0:
            std_series = df[numeric_cols].std()
            if not std_series.isna().all():
                max_std_col = std_series.idxmax()
                min_std_col = std_series.idxmin()
                if pd.notna(max_std_col) and pd.notna(min_std_col):
                    insights.append(
                        f"ğŸ“Š **æ³¢åŠ¨æ´å¯Ÿ**ï¼šæ•°æ®ä¸­ã€{max_std_col}ã€‘çš„ç¦»æ•£ç¨‹åº¦æœ€å¤§ï¼Œè¯´æ˜ä¸ªä½“å·®å¼‚æœ€æ˜¾è‘—ï¼›è€Œã€{min_std_col}ã€‘åˆ†å¸ƒæœ€ä¸ºé›†ä¸­ç¨³å®šã€‚")

            # 2. å…³è”æ´å¯Ÿ
            if len(numeric_cols) >= 2:
                corr_df = df[numeric_cols].corr().abs()
                for i in range(len(corr_df.columns)):
                    corr_df.iloc[i, i] = 0.0

                if not corr_df.isna().all().all():
                    max_corr = corr_df.max().max()
                    if pd.notna(max_corr) and max_corr > 0.5:
                        safe_matrix = corr_df.to_numpy()
                        row_idx, col_idx = np.unravel_index(np.nanargmax(safe_matrix), safe_matrix.shape)
                        var1, var2 = corr_df.index[row_idx], corr_df.columns[col_idx]
                        insights.append(
                            f"ğŸ”— **å¼ºå…³è”å‘ç°**ï¼šç³»ç»Ÿæ£€æµ‹åˆ°ã€{var1}ã€‘ä¸ã€{var2}ã€‘å­˜åœ¨è¾ƒå¼ºçš„çº¿æ€§ç›¸å…³æ€§ï¼ˆr={round(max_corr, 2)}ï¼‰ã€‚è¿™è¡¨æ˜å½“å…¶ä¸­ä¸€ä¸ªæŒ‡æ ‡å˜åŒ–æ—¶ï¼Œå¦ä¸€ä¸ªå¾€å¾€ä¹Ÿä¼šéšä¹‹å‘ˆç°è§„å¾‹æ€§æ³¢åŠ¨ã€‚")

        insights.append("ğŸ’¡ **åˆ†æå»ºè®®**ï¼šå»ºè®®ç»“åˆå³ä¾§çš„é›·è¾¾å›¾è¿›è¡Œä¸ªä½“ä¼˜åŠ¿è¯Šæ–­ï¼Œå¹¶åˆ©ç”¨ t æ£€éªŒè¿›ä¸€æ­¥æ¢ç´¢ä¸åŒç¾¤ä½“é—´çš„å·®å¼‚ã€‚")
        return jsonify({"status": "success", "data": insights})
    except Exception as e:
        return jsonify({"status": "error", "message": f"ç®—æ³•è¿ç®—å¼‚å¸¸: {str(e)}"}), 500


@analysis_bp.route('/api/predict', methods=['POST'])
def predict_data():
    """æœºå™¨å­¦ä¹ é¢„æµ‹å¼•æ“ (Random Forest Regressor)"""
    try:
        data = request.json
        filename = data.get('filename')
        target_col = data.get('target_col')
        feature_cols = data.get('feature_cols', [])

        if not filename or not target_col or len(feature_cols) == 0:
            return jsonify({"status": "error", "message": "å‚æ•°ä¸å®Œæ•´"}), 400

        df = read_df(os.path.join(UPLOAD_FOLDER, filename))

        # æå–æœ‰æ•ˆæ•°æ®å¹¶æ¸…ç†ç©ºå€¼
        df_clean = df[[target_col] + feature_cols].dropna()
        if len(df_clean) < 10:
            return jsonify({"status": "error", "message": "æœ‰æ•ˆæ•°æ®é‡å¤ªå°‘ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹"}), 400

        X = df_clean[feature_cols]
        y = df_clean[target_col]

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # è®­ç»ƒéšæœºæ£®æ—å›å½’æ¨¡å‹
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
        r2 = r2_score(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        importances = model.feature_importances_.tolist()

        # ç»„è£…æ•£ç‚¹å›¾æ•°æ®
        scatter_data = [[float(actual), float(pred)] for actual, pred in
                        zip(y_test.tolist()[:100], y_pred.tolist()[:100])]

        return jsonify({
            "status": "success",
            "data": {
                "r2": round(r2, 4),
                "mse": round(mse, 4),
                "features": feature_cols,
                "importances": [round(i * 100, 2) for i in importances],
                "scatter": scatter_data
            }
        })
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


# ==========================================
# æ¨¡å—å››ï¼šä¼ä¸šçº§å®‰å…¨åè®®
# ==========================================

@analysis_bp.route('/api/mask', methods=['POST'])
def mask_data():
    """ä¼ä¸šçº§éšç§æ•°æ®è„±æ•åŠ å¯†"""
    try:
        filename = request.json.get('filename')
        if not filename:
            return jsonify({"status": "error", "message": "æœªæ‰¾åˆ°æ–‡ä»¶"}), 400

        df = read_df(os.path.join(UPLOAD_FOLDER, filename))
        masked_cols = []

        # æ ¸å¿ƒåŠ å¯†ç®—æ³•å‡½æ•°
        def mask_val(x):
            x = str(x).strip()
            if x in ['nan', 'None', '']:
                return x
            if len(x) <= 2:
                return x[0] + '*'
            elif len(x) == 3:
                return x[0] + '*' + x[-1]
            else:
                return x[:4] + '****'

        # æ™ºèƒ½æ‰«ææ•æ„Ÿåˆ—
        for col in df.columns:
            col_name = str(col).lower()
            if any(kw in col_name for kw in ['å', 'name', 'å·', 'id', 'æ‰‹æœº', 'ç”µè¯', 'èº«ä»½']):
                masked_cols.append(col)
                df[col] = df[col].apply(mask_val)

        # å¦å­˜è„±æ•æ–‡ä»¶
        masked_filename = "masked_" + filename
        df.to_csv(os.path.join(UPLOAD_FOLDER, masked_filename), index=False, encoding='utf-8-sig')

        return jsonify({
            "status": "success",
            "data": {
                "masked_filename": masked_filename,
                "masked_cols": masked_cols
            }
        })
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500